---
layout: post
mathjax: true
comments: true
title:  "Survey on Self-driving Technology"
---
After reading the following two survey papers, I decided to include other self-driving papers I read into the mix. They are not end2end, most deas with either perception or planning.

* TOC
{:toc}

```
"A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles" by
Brian Paden, Michal Cap, Sze Zheng Yong, Dmitry Yershov, Emilio Frazzoli
2016
```

```
"Perception, Planning, Control, and Coordination for Autonomous Vehicles" by
Pendleton, Scott
Andersen, Hans
Du, Xinxin
Shen, Xiaotong
Meghjani, Malika
Eng, You
Rus, Daniela
Ang, Marcelo.
2017, Journal of Machines.
```


# Perception

## Lidar

Lidar is the most reliable way to perform segmentation and recognition. The noisy and sparse point clouds is computationlly challenging to recognize. There are cases even human having trouble make sense of them. Nowadays, deep learning is mostly capable. Geometric methods are good at special cases, eg curb line fitting based on edge method, region based clustering method for vehicle, ground plane segmentation, graph cut with CRF or MRF.


### Ground Plane Segmentation

For ground plane segmentation, RANSAC and Hough Transform are used. To deal with non planar ground or slope, more flexible constraints such as quadratic form, piecewise (5m strip).

## Vision

### Lane Line Detection

A vision based method typically consists of
* Lane marking detection by edge detection or SVM classifier or box filter. Can't distinguish other markings on road.
* Fit lane marking into lines, parabolas, zigzag etc
* Improve estimation continuity by Kalman filter and Particle filter [68].
* Vehicle pose estimation [88, 95]
Due to low texture of road surface, stereo is not widely used to recover depth.
Mathworks has a good [reference design](https://www.mathworks.com/help/visionhdl/examples/lane-detection.html).


### Road Surface Detection

So that vehicle knows where to drive.
Traditionally, most are featured (eg HOG) based classifier on pixel or patch, while adding context information. Deep learning segmentation is the way to go here.

### Object Detection

Detect Vehicle, people, obstacles. All 2d bbox method, such as faster-rcnn, retinanet etc can be used.

#### Monocular 3D Object Detection

Paper "Monocular 3D Object Detection for Autonomous Driving", cvpr 2016, use single image to estimate 3d objects. Reference [10] use stereo imagery to create 3d proposals. RCNN 2d bbox is not accurate.
proposals are generated by exhaustively
placing 3D bounding boxes on the ground-plane
and scoring them via simple and efficiently computable image features

a fixed ground plane as camera is fixed, grid 0.2cm.
generate 3d proposal, then now it is easy to project them to the image. 14k candidate boxes, reduce to 28% eg if box only contains road pixels.
socring: use a simple svm scoring function for proposal. eg feature could be how many car pixel (use a seperate segmentation network for that) is in my image projection. use integral image in most of these counting.
cnn scoring: use modified fast rcnn to score the projected region for class, bbox offset (guess this is the 3d bbox), orientation (front or back only).

AP get to 88% on cars, 66% on pedestrians, 66% cyclists. question is will a small car estimated to be further way than it should be?


### Sensor Fusion

Fuse Lidar and vision at pixel level by mapping 3d lidar point to image plane, then adding image intension to lidar point cloud. Unclear how to deal with image occlusion.

#### 3D Object Detection with Lidar and Camera

"Multi-View 3D Object Detection Network for Autonomous Driving", 2017.
frontview, bird view, camera.

similar to faster-rcnn has a 3d region proposal net, then project 3d box to each view, use roi pooling to extract feature for the proposal, then regress class and 3d box 8 corners. region proposal net is using bird eye view. The region network use image, front view and bird view feature. by the way, front view is cyclindal view using two angles. the bird view is x, y as coordinate, the z, density, reflectance as channels. Also divide z into M slices as M channel to avoid oclusion on Z.

##### voting for votes
using 3d grid to represent point cloud, and efficient svm based sliding 3d window detector.

discretize space into fixed 3d grid, each grid with points inside compute a feature vector, create a fixed size sliding 3d box and use svm for cells inside to detect cars, since cars can rotate, we discretize 360 into N orientation bins (assuming cars only rotate along the vertical as it is on groud), and does N such sliding window procedures.

3d point cloud is very sparse. sparse linear sliding window is equalent to voting to the locations whose sliding window box include this cell. votes are different based on relative location.

ech cell's feature is 6 numbers. eval on kitti 3d bounding box (given in  canonical
orientation). iterate 20 times to perform hard negative mining. trained a linear svm. efficient half a second per image for inference.

##### Vote3Deep
same 3d grid, but now use 3 layer 3d convolution.

use sparse multilayer cnn similar to the above paper. still feature centric convolution is used to only compute cells with 3d points. for example kernel size is 3x3x3. It will be interesting to see whether the full 3d structure is setup, and only non empty points are calculated.

##### Vehicle Detection from 3D Lidar Using Fully Convolutional Network

This use front view projection (ie cyclindal map), theta, phi 2d coordinate, 2 chnnale: d and z image. architecture is convolution, then deconv. each point says belong to a vehicle or not, and also the vehcile's 3d bounding box. I don't understand the normalized 3d box to keep rotation invariance. otherwise standard. this use all negative points but weighted low.


## Localization

The lidar reflectivity map on road surface method from Stanford.

### Tracking with Different Motion Models

"Comparison and evaluation of advanced motion models for vehicle tracking" 2008. Compared UKF tracking with fused GPS (position) and odometry (velocity and yaw rate) measurement using motion models constant velocity (CV), constant turn rate and velocity (CTRV), constant turn rate and accleration (CTRA), constant curvature and velocity (CCA). The ground truth trajectory is from a RTK DGPS (accurate to a few centimeters). There is no external control. Paper find CTRA is most accurate, the more complex CCA is simiar, the first two simpler methods are inferior.

### GraphSLAM with Lidar Reflectivity map
Paper "Map-Based Precision Vehicle Localization in Urban Environments" by Jesse Levinson and Sebastian Thrun 2007.

[15] has vehicel dynamics

graphslam approach. motion edge is by imu [15], observation is ground plane reflecitivty map, added gps contraints. Since this is slam, trick is to integrate map out, if two sequence has overlap, a local alignment is searched to determine the delta and added to the constraint. This is the loop closure problem. The result is an adjusted robot path that resolves inconsistencies at intersections.

the local map matching: a linear correlation field is computed between these maps, for different x-y offsets between these images.

After pose is computed, map is generated by optimize for equation (10) page 4. this is fast. map is stored by breaking into squares and only store squres with data ( I guess the lane marks).

online localization is by particle filter. measurement is based on Pearson product-moment correlation of h(m, x) with observation.

figure 5 of burlingame obtained by 32 loops.

It should be possible to do this with a camera, it has motion sensors.

### Lidar Localzation using Probabilistic Maps

"Robust Vehicle Localization in Urban Environments Using Probabilistic Maps" by Jesse Levinson and Sebastian Thrun 2010.

main change:
1. map and radar scan is a grid cells of intensity average and variance.
2. use histogram filter rely on accurate gps/imu data.

grid cell representation: each with avg and variance of reflectivity.
use histogram filter and compare two map of grid cells.

local map matching: alignment offsets between matched poses are computed using iterative closest point

calibrate each beam so that beam has same brightness, for each beam j output a, the corrected value is the avg of all other beams on the same cells as j when j output a.

map: 15x15 cm of ground with two channels. now don't need to remove dynamci objects, maps cells containing them will have high variance.

localization is actually use 2d x/y histogram filter around gps location.
vehicle dynamics is in [9]. laser scan grid is build a rolling grid from accumlated sensor data instead of a single scan, that way can accumulate variance. The posterior of x,y is in equation 6 in terms of grid cells of avg, variance between map and observation. in this paper does not talk about orientation of car. just directly compare each cell and multiply gaussian.

### Visual Localzation with Lidar Maps

"Visual Localization within LIDAR Maps for Automated Urban Driving" by Ryan W. Wolcott and Ryan M. Eustice 2014.

visual slam is hard due to eg weather conditions.

Use lidar to generate ground mesh texture (not planar). use camera image to match with a view of lidar in localization.

use iSLAM to compute poseGraph, then compute triangle groundmesh.
use mesh-surface as opposed to a strict planar texture because road is not flat.
they tried full 3d mesh structure, but does not improve much.

use opengl to render for a camera $[R \mid t]$. at runtime, generate n_x * n_y views of the map, also generate n_theta views of the source image, compare them. but how can you warp a image 360 degree? I can understand you can opengl a 3d structure. use Normalized Mutual Information instead of correlation. (H(A) + H(B)) / H(AB). quesiton is the 3d model from lidar is in infrared, will it match camera image well? I guess texture intensity should be fine.

use EKF to localize. not clear how this is related with image registration.


### Visual SLAM using Bundle Adjustment

"ORB-SLAM: a Versatile and Accurate Monocular SLAM System"

use orb to track each frame to localize camera, select key frams to perform local mapping using bundle adjustment to optimize for local camera pose. when loop is detected, form essiental graph (keyframe with sparse edges) to distribute the error. accurate then lsd-slam and ptam. works for indoor and on car.

also use bag of words to index frame for global relocalization.

1) the tracking to localize the camera with every frame by finding feature matches to the local map and minimizing the reprojection error applying motion-only BA, 2) the local mapping to manage the local map and optimize it, performing local BA, 3) the loop closing to detect large loops and correct the accumulated drift by performing a pose-graph optimization. This thread launches a fourth thread to perform full BA after the pose-graph optimization, to compute the optimal structure and motion solution.

Rely heavily on bundle adjustment. accuracy is several meters off.

"ORB-SLAM2" is a followup paper.

support stereo and depth camera. model stereo keypoint by (x,y, x'), where x' is location in the right image. For depth camera, transform the depth into x'. Then in bundle adjustment, x' become another constraint. stereo helps at initialization step. It can provide scale and remove scale drift when loop close.

stereo is more accurate.

### Lane HD map using Cameras

"LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments" 2018.

This work is slam with cameras. use sfm to extract 3d point cloud. cnn work directly on raw image. post fiting is also on raw image. Only at sfm determine 3d location.

hd map has lane info (lane positions, boundary, lane line types, orientation), eg ny has the service lane seperated.
Directly take image, so lanes converge.
LineNet has 6 branch for lane mask, position, distance, line type, orientation. This works for all lanes, boundaries. type has double line, white, yellow etc.
Zoom module, take crop and determine locally line type.
cnn gives lane point, post processing to DBSCAN [7] cluster points and fit 3 degree polynomail.

the pipeline is, this is similar to a slam approach, where sfm links poses, the ground plane and lane lines becomes map. Figure 10 is the composed photos
1. use app to take shots every 5-20 meters, gps erros is about 5 meters
2. detect lanes on each image
3. openSfm to identify ground plane point clouds, fit ground to plane
4. lanes from 2 project to ground from 3, merged using same DBSCAN cluster method to long lanes.
5. compare with ground truth from laser data, error < 31cm.


# Planning

Route planning picks sequence of road segments. Behavorial planner generate discrete motion goals (location, speed) adherence to rules of road, eg desired lane and speed command, driving down lane reach (x,y). Motion planner generates trajectory to reach local goal. Planning is dynamic, eg estimate time of collision.

## Route Planning

Graph search using Dijkastra or A* search. For large network see [196].

## Behavorial Planning

Early work use finite state machines. Check precedence and clearance before making transitions. Clearance checks time to collision. Precedence for example checks stop sign order. Need to enlarge the set of rules and check deadlock, livelock stituation using LTL.

## Motion Planning

Discrete configuration space, use graph search, eg A* to find solution. Technics used are LTL [211], POMDP, MOMDP due to uncertainty of environment. SARSOP [216] is making POMDP practical. One example of POMDP is [213].

Sampling based planning PRM, RRT.

Assume other's has constant velocity, then rapid replanning if things change, or enlarge obstale, can also use more liberal assumption, eg bounded velocity and bounded accerlation. Trajectory in space time becomes volumes of different shapes.

Planning should consider some amount of kinematic and dynamic constraints, eg turning radius (Reeds-shepp curves), such methods include [243, 244, 245, 248].

Replanning is necessary in a dynamic environment, eg plan biased to previously generate waypoints, or rewire existing search tree.

### Hybrid A* Planning Paper
It is used in Stanford Junior "Practical Search Techniques in Path Planning for Autonomous Driving". hybrid-state A*: 160x160 1m x5degree discretization for a drivable search, state is contiguous.
also does a Reed-and-Sheep path which is long to the goal and accept it if no clision
To avoid obstacle use a Voronoi field potential in CG local optimization after A*.
then interpolate through sampling and CG.
This is a driveable x,y,theta planner, can go foward and backward, can do parking, u turn etc.
hybrid-state a* uses the first reached state inside a grid as the cost of that grid, you explore a fixed set of orientation, and use reed-sheep path if getting closer, which fits several circles and straight lines to reach destination. There is also a modified potential field. This [blog post](https://blog.habrador.com/2015/11/explaining-hybrid-star-pathfinding.html) has a good demo.

### Boss Motion Planner

Paper "Motion Planning in Urban Environments", 2008.

Onroad driving parameters are curvature profile using second order spline (3 parameters only). Speed profile is chosen among simple piecewise linear functions. Instead of direct fit a path. Use motion model to compute the trajectory for evaluation through numerical integration. Optimization is Newton's method. Partial derivatives is computed numerically. Multiple trajectries with slightly different lateral offset is generated and evaluated for collision and offset to centerline. Precompute 5D lookup table to initialize optimizer. I guess this paramerization + motion model makes the trajectory drivable, only 3 parameters should make optimization fast.
![Boss Profiles](/assets/bossprofiles.png)

Unstructured driving is using 4d lattice planner (Anytime Dynamic A*) algorithm on $(x, y, \theta, v)$. Cost heuristic is max(no obstcale offline cost, online 2d grid based dijkstra). For dynamic obstacles, its short term trajectory is encoded into the combined cost map as a hard constraint, also around them using high-cost region. This planner is also invoked for difficult maneuvers: U turn, blocked lane. Once lattice planner is done, a similar onroad planner is used to generate trajectries following the path. It seems this involves many short trajectries terminating on the lattice path.

### MOMDP Offline Planner

Paper "Intention-Aware Motion Planning", 2014.

This is more like a behavioral planner than motion planner.
MOMDP: mixed observability Markov decision process. Offline planner construct a motion model for each agent intention. Robot solves momdp for a policy. online: execute policy over a set of intentions based on observed behavior. Robot only has an observation not the state of agents. agent has observes everything, it has its own policy conditioned on its intention. agent policy can be computed by MDP. Use SARSOP to compute the offline policy.

For pedestrian interaction experiment,
1mx1m grid, time step is 1 second. pedestrian goal is not known.
Compared with Bayes-ML: precompute mdp with known pedestrian goal, online bayesian inference on the pedestrain goal based on observation. It choose action based on the most likely intention.
Bayes-ML cause accidents, because motion is noisy, at the crossing, if it believe the intention is slightly lower to cross, it will keep current speed and when it is clear peron is actually corssing, it is too late. robot choose accelerate in view of the possibility of accident. MOMDP resolves intention only when it is necessary.

For vehicle intersection behavior experiment, assuming 4 driving behavior of vehicle A. Reward function tradeoff accident rate vs clearing time. Measure the performance of MOMDP vs MDP assuming one of the behaviors. Guess the behavior of A affects its state transitions and policy. some game aspect, one impatient behaivor A will accelerate to turn left if R slows down. PODMP models other driving behaviros is interesting.

# Control

Follow planned trajectory, actuator and correct tracking errors.

## PID control
PID error signal $u(t) = k_d \dot(e) + k_p e + k_i \int e(t) dt$. Feedback control has delayed response to errors. Feedfoward eg fit a circle of upcoming trajectory.

## MPC

Discretize time, Optimize a cost function with motion constraints.


## Trajectory Generation and Tracking

[277] is a good paper on control.

Pure pursuit path trackign algorithm to track a lookahead waypoint, also the Stanford Stanley method. Both are stable control algorithm. More complex models for high speed.

# Cooperation

Wireless communication between vehicle, eg DSRC works up to 1000 meters. A lot of security issues. The internet of cars may be huge besides driving.